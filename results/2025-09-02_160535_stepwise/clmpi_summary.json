{
  "model": "mistral:7b",
  "timestamp": "2025-09-02 21:20:56",
  "evaluation_run": "2025-09-02_160535_stepwise",
  "clmpi_scores": {
    "clmpi_01": 0.6448441558441558,
    "clmpi_25": 16.121103896103893,
    "clmpi_100": 64.48441558441557
  },
  "component_scores": {
    "accuracy": {
      "score": 0.0,
      "weight": 0.25,
      "contribution": 0.0
    },
    "context": {
      "score": 1.0,
      "weight": 0.2,
      "contribution": 0.2
    },
    "coherence": {
      "score": 0.9363636363636363,
      "weight": 0.2,
      "contribution": 0.18727272727272726
    },
    "fluency": {
      "score": 0.9428571428571428,
      "weight": 0.2,
      "contribution": 0.18857142857142858
    },
    "efficiency": {
      "score": 0.45999999999999996,
      "weight": 0.15,
      "contribution": 0.06899999999999999
    }
  },
  "weights_used": {
    "accuracy": 0.25,
    "contextual_understanding": 0.2,
    "coherence": 0.2,
    "fluency": 0.2,
    "performance_efficiency": 0.15
  },
  "detailed_results": {
    "accuracy": {
      "metric": "accuracy",
      "model": "mistral:7b",
      "timestamp": "2025-09-02 21:20:56",
      "exact_match": 0.0,
      "f1_score": 0.0,
      "total_questions": 1,
      "generation_profile": "deterministic",
      "dataset_path": "prompts/accuracy.json"
    },
    "context": {
      "metric": "contextual_understanding",
      "model": "mistral:7b",
      "timestamp": "2025-09-02 21:23:22",
      "exact_match": 1.0,
      "f1_score": 1.0,
      "context_similarity": 0.0,
      "combined_score": 1.0,
      "total_conversations": 1,
      "generation_profile": "deterministic",
      "dataset_path": "prompts/context.json"
    },
    "coherence": {
      "metric": "coherence",
      "model": "mistral:7b",
      "timestamp": "2025-09-02 21:23:50",
      "exact_match": 0.9363636363636363,
      "f1_score": 0.9363636363636363,
      "coherence_score": 0.9363636363636363,
      "total_prompts": 1,
      "generation_profile": "creative",
      "dataset_path": "prompts/coherence.json"
    },
    "fluency": {
      "metric": "fluency",
      "model": "mistral:7b",
      "timestamp": "2025-09-02 21:24:03",
      "exact_match": 0.9428571428571428,
      "f1_score": 0.9428571428571428,
      "fluency_score": 0.9428571428571428,
      "total_prompts": 1,
      "generation_profile": "creative",
      "dataset_path": "prompts/fluency.json"
    },
    "efficiency": {
      "metric": "efficiency",
      "model": "mistral:7b",
      "timestamp": "2025-09-02 21:23:32",
      "efficiency": 0.45999999999999996,
      "accuracy_score": 1.0,
      "inference_time_seconds": 8.082312107086182,
      "model_size_mb": 4170.238220214844,
      "peak_memory_mb": 4.48828125,
      "cpu_usage_percent": 0.0,
      "success": true,
      "generation_profile": "deterministic",
      "dataset_path": "prompts/efficiency_tasks.json"
    }
  },
  "evaluation_metadata": {
    "generation_profiles_used": {
      "accuracy": "deterministic",
      "context": "deterministic",
      "coherence": "creative",
      "fluency": "creative",
      "efficiency": "deterministic"
    },
    "datasets_used": {
      "accuracy": "prompts/accuracy.json",
      "context": "prompts/context.json",
      "coherence": "prompts/coherence.json",
      "fluency": "prompts/fluency.json",
      "efficiency": "prompts/efficiency_tasks.json"
    }
  }
}