# CLMPI Model Configuration
# Defines models to evaluate and their parameters

# Global Evaluation Weights (must sum to 1.0)
# ACC: Accuracy - factual correctness [0,1]
# CON: Contextual Understanding - multi-turn conversations [0,1] 
# COH: Coherence - logical flow [0,1]
# FLU: Fluency - language quality [0,1]
# EFF: Performance Efficiency - resource usage [0,1]
evaluation_weights:
  accuracy: 0.25          # 25% - Factual correctness
  contextual_understanding: 0.20  # 20% - Multi-turn conversations
  coherence: 0.20         # 20% - Logical flow and structure
  fluency: 0.20           # 20% - Language quality
  performance_efficiency: 0.15   # 15% - Resource efficiency

# Prompt Sets to Enable
# Each maps to a specific evaluation dimension with principled benchmark datasets
prompt_sets:
  accuracy:
    - "accuracy.json"  # GSM8K mathematical reasoning
  contextual_understanding:
    - "context.json"  # SQuAD passage comprehension
  coherence:
    - "coherence.json"  # HellaSwag sentence completion
  fluency:
    - "fluency.json"  # CoLA grammatical acceptability
  performance_efficiency:
    - "efficiency_tasks.json"  # Computational complexity testing

# Model Definitions
# Add models here as needed. All models must be available via Ollama
# Generation settings are standardized via generation_config.yaml
models:
  phi3_3_8b:
    ollama_name: "phi3:3.8b"
    timeout_seconds: 30

# Evaluation Parameters
evaluation:
  samples_per_task: 5      # Number of prompts per task type
  random_seed: 42          # Fixed seed for reproducibility
  save_raw_responses: true
  generate_visualizations: true
