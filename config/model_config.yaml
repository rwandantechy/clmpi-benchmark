# CLMPI Model Configuration
# Defines models to evaluate and their parameters

# Global Evaluation Weights (must sum to 1.0)
# ACC: Accuracy - factual correctness [0,1]
# CON: Contextual Understanding - multi-turn conversations [0,1] 
# COH: Coherence - logical flow [0,1]
# FLU: Fluency - language quality [0,1]
# EFF: Performance Efficiency - resource usage [0,1]
evaluation_weights:
  accuracy: 0.25          # 25% - Factual correctness
  contextual_understanding: 0.20  # 20% - Multi-turn conversations
  coherence: 0.20         # 20% - Logical flow and structure
  fluency: 0.20           # 20% - Language quality
  performance_efficiency: 0.15   # 15% - Resource efficiency

# Prompt Sets to Enable
# Each maps to a specific evaluation dimension
prompt_sets:
  accuracy:
    - "classification_tasks.json"
    - "reasoning_tasks.json"
  contextual_understanding:
    - "contextual_tasks.json"
  coherence:
    - "contextual_tasks.json"  # Multi-turn conversations test coherence
  fluency:
    - "classification_tasks.json"  # All responses evaluated for fluency
  performance_efficiency:
    - "classification_tasks.json"  # All prompts used for efficiency measurement

# Model Definitions
# Add/remove models here. All models must be available via Ollama
models:
  phi3:mini:
    ollama_name: "phi3:mini"
    max_tokens: 1000
    temperature: 0.1
    timeout_seconds: 30
    
  mistral:
    ollama_name: "mistral"
    max_tokens: 1000
    temperature: 0.1
    timeout_seconds: 45
    
  llama2:7b-chat:
    ollama_name: "llama2:7b-chat"
    max_tokens: 1000
    temperature: 0.1
    timeout_seconds: 60
    
  gemma:2b:
    ollama_name: "gemma:2b"
    max_tokens: 1000
    temperature: 0.1
    timeout_seconds: 20

# Evaluation Parameters
evaluation:
  samples_per_task: 5      # Number of prompts per task type
  random_seed: 42          # Fixed seed for reproducibility
  save_raw_responses: true
  generate_visualizations: true
